{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5576042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The webpage \"India Real Estate Property Site - Buy Sell Rent Properties Portal - 99acres.com\" did get fully laoded.\n",
      "\n",
      "The webpage \"Property in Bangalore - Real Estate in Bangalore\" did get fully laoded.\n",
      "\n",
      "Timeout because we have uncovered all filters.\n",
      "\n",
      "Timeout while clicking on \"Next Page\".\n",
      "\n",
      "Timeout because we have navigated all the 2 pages.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# ----- SCRAPING THE DATA -----\n",
    "\n",
    "def wait_for_page_to_load(driver, wait):\n",
    "\ttitle = driver.title\n",
    "\ttry:\n",
    "\t\twait.until(\n",
    "\t\t\tlambda d: d.execute_script(\"return document.readyState\") == \"complete\"\n",
    "\t\t)\n",
    "\texcept:\n",
    "\t\tprint(f\"The webpage \\\"{title}\\\" did not get fully laoded.\\n\")\n",
    "\telse:\n",
    "\t\tprint(f\"The webpage \\\"{title}\\\" did get fully laoded.\\n\")\n",
    "\n",
    "\n",
    "# options\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--disable-http2\")\n",
    "chrome_options.add_argument(\"--incognito\")\n",
    "chrome_options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "chrome_options.add_argument(\"--ignore-certificate-errors\")\n",
    "chrome_options.add_argument(\"--enable-features=NetworkServiceInProcess\")\n",
    "chrome_options.add_argument(\"--disable-features=NetworkService\")\n",
    "chrome_options.add_argument(\n",
    "    \"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.63 Safari/537.36\"\n",
    ")\n",
    "\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "driver.maximize_window()\n",
    "\n",
    "# explicit wait\n",
    "wait = WebDriverWait(driver, 5)\n",
    "\n",
    "# accessing the target webpage\n",
    "url = \"https://www.99acres.com/\"\n",
    "driver.get(url)\n",
    "wait_for_page_to_load(driver, wait)\n",
    "\n",
    "# identify and enter text into search bar\n",
    "try:\n",
    "\tsearch_bar = wait.until(\n",
    "\t\tEC.presence_of_element_located((By.XPATH, '//*[@id=\"keyword2\"]'))\n",
    "\t)\n",
    "except:\n",
    "\tprint(\"Timeout while locating Search Bar.\\n\")\n",
    "else:\n",
    "\tsearch_bar.send_keys(\"Bangalore\")\n",
    "\ttime.sleep(2)\n",
    "\n",
    "# selecting valid option from list\n",
    "try:\n",
    "\tvalid_option = wait.until(\n",
    "\t\tEC.element_to_be_clickable((By.XPATH, '//*[@id=\"0\"]'))\n",
    "\t)\n",
    "except:\n",
    "\tprint(\"Timeout while locating valid search option.\\n\")\n",
    "else:\n",
    "\tvalid_option.click()\n",
    "\ttime.sleep(2)\n",
    "\n",
    "# click on Search button\n",
    "try:\n",
    "\tsearch_button = wait.until(\n",
    "\t\tEC.element_to_be_clickable((By.XPATH, '//*[@id=\"searchform_search_btn\"]'))\n",
    "\t)\n",
    "except:\n",
    "\tprint(\"Timeout while clicking on \\\"Search\\\" button.\\n\")\n",
    "else:\n",
    "\tsearch_button.click()\n",
    "\twait_for_page_to_load(driver, wait)\n",
    "\n",
    "# adjust the Budget slider\n",
    "try:\n",
    "\tslider = wait.until(\n",
    "\t\tEC.element_to_be_clickable((By.XPATH, '//*[@id=\"budgetLeftFilter_max_node\"]'))\n",
    "\t)\n",
    "except:\n",
    "\tprint(\"Timeout while clicking on Budget slider circle.\\n\")\n",
    "else:\n",
    "\tactions = ActionChains(driver)\n",
    "\t(\n",
    "\t\tactions\n",
    "\t\t.click_and_hold(slider)\n",
    "\t\t.move_by_offset(-73, 0)\n",
    "\t\t.release()\n",
    "\t\t.perform()\n",
    "\t)\n",
    "\ttime.sleep(2)\n",
    "\n",
    "# filter results to show genuine listings\n",
    "# 1. Verified\n",
    "verified = wait.until(\n",
    "\tEC.element_to_be_clickable((By.XPATH, '/html[1]/body[1]/div[1]/div[1]/div[1]/div[4]/div[3]/div[1]/div[3]/section[1]/div[1]/div[1]/div[1]/div[1]/div[1]/div[1]/div[3]/span[2]'))\n",
    ")\n",
    "verified.click()\n",
    "time.sleep(1)\n",
    "\n",
    "# 2. Ready To Move\n",
    "ready_to_move = wait.until(\n",
    "\tEC.element_to_be_clickable((By.XPATH, '/html[1]/body[1]/div[1]/div[1]/div[1]/div[4]/div[3]/div[1]/div[3]/section[1]/div[1]/div[1]/div[1]/div[1]/div[1]/div[1]/div[5]/span[2]'))\n",
    ")\n",
    "ready_to_move.click()\n",
    "time.sleep(1)\n",
    "\n",
    "# moving to the right side to unhide remaining filters\n",
    "while True:\n",
    "\ttry:\n",
    "\t\tfilter_right_button = wait.until(\n",
    "\t\t\tEC.presence_of_element_located((By.XPATH, \"//i[contains(@class,'iconS_Common_24 icon_upArrow cc__rightArrow')]\"))\n",
    "\t\t)\n",
    "\texcept:\n",
    "\t\tprint(\"Timeout because we have uncovered all filters.\\n\")\n",
    "\t\tbreak\n",
    "\telse:\n",
    "\t\tfilter_right_button.click()\n",
    "\t\ttime.sleep(10)\n",
    "\n",
    "# 3. With Photos\n",
    "with_photos = wait.until(\n",
    "\tEC.element_to_be_clickable((By.XPATH, '/html[1]/body[1]/div[1]/div[1]/div[1]/div[4]/div[3]/div[1]/div[3]/section[1]/div[1]/div[1]/div[1]/div[1]/div[2]/div[1]/div[6]/span[2]'))\n",
    ")\n",
    "with_photos.click()\n",
    "time.sleep(1)\n",
    "\n",
    "# 4. With Videos\n",
    "with_videos = wait.until(\n",
    "\tEC.element_to_be_clickable((By.XPATH, '/html[1]/body[1]/div[1]/div[1]/div[1]/div[4]/div[3]/div[1]/div[3]/section[1]/div[1]/div[1]/div[1]/div[1]/div[2]/div[1]/div[7]/span[2]'))\n",
    ")\n",
    "with_videos.click()\n",
    "time.sleep(3)\n",
    "\n",
    "# navigate pages and extract data\n",
    "data = []\n",
    "page_count = 0\n",
    "while True:\n",
    "\tpage_count += 1\n",
    "\ttry:\n",
    "\t\tnext_page_button = driver.find_element(By.XPATH, \"//a[normalize-space()='Next Page >']\")\n",
    "\texcept:\n",
    "\t\tprint(f\"Timeout because we have navigated all the {page_count} pages.\\n\")\n",
    "\t\tbreak\n",
    "\telse:\n",
    "\t\ttry:\n",
    "\t\t\tdriver.execute_script(\"window.scrollBy(0, arguments[0].getBoundingClientRect().top - 100);\", next_page_button)\n",
    "\t\t\ttime.sleep(20)\n",
    "\t\n",
    "\t\t\t# scraping the data\n",
    "\t\t\trows = driver.find_elements(By.CLASS_NAME, \"tupleNew__TupleContent\")\n",
    "\t\t\tfor row in rows:\n",
    "\t\t\t\t# property name\n",
    "\t\t\t\ttry:\n",
    "\t\t\t\t\tname = row.find_element(By.CLASS_NAME, \"tupleNew__headingNrera\").text\n",
    "\t\t\t\texcept:\n",
    "\t\t\t\t\tname = np.nan\n",
    "\n",
    "\t\t\t\t# property location\n",
    "\t\t\t\ttry:\n",
    "\t\t\t\t\tlocation = row.find_element(By.CLASS_NAME, \"tupleNew__propType\").text\n",
    "\t\t\t\texcept:\n",
    "\t\t\t\t\tlocation = np.nan\n",
    "\n",
    "\t\t\t\t# property price\n",
    "\t\t\t\ttry:\n",
    "\t\t\t\t\tprice = row.find_element(By.CLASS_NAME, \"tupleNew__priceValWrap\").text\n",
    "\t\t\t\texcept:\n",
    "\t\t\t\t\tprice = np.nan\n",
    "\n",
    "\t\t\t\t# property area and bhk\n",
    "\t\t\t\ttry:\n",
    "\t\t\t\t\telements = row.find_elements(By.CLASS_NAME, \"tupleNew__area1Type\")\n",
    "\t\t\t\texcept:\n",
    "\t\t\t\t\tarea, bhk = [np.nan, np.nan]\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tarea, bhk = [ele.text for ele in elements]\n",
    "\t\t\t\t\t\n",
    "\t\t\t\tproperty = {\n",
    "\t\t\t\t\t\"name\": name,\n",
    "\t\t\t\t\t\"location\": location,\n",
    "\t\t\t\t\t\"price\": price,\n",
    "\t\t\t\t\t\"area\": area,\n",
    "\t\t\t\t\t\"bhk\": bhk\n",
    "\t\t\t\t}\n",
    "\t\t\t\tdata.append(property)\n",
    "\t\t\t\n",
    "\t\t\twait.until(\n",
    "\t\t\t\tEC.element_to_be_clickable((By.XPATH, \"//a[normalize-space()='Next Page >']\"))\n",
    "\t\t\t).click()\n",
    "\t\t\ttime.sleep(10)\n",
    "\t\texcept Exception as e:\n",
    "\t\t\tprint(f'Exception: {e}')\n",
    "\t\t\tprint(\"Timeout while clicking on \\\"Next Page\\\".\\n\")\n",
    "\n",
    "# scraping data from the last page\n",
    "rows = driver.find_elements(By.CLASS_NAME, \"tupleNew__TupleContent\")\n",
    "for row in rows:\n",
    "\t# property name\n",
    "\ttry:\n",
    "\t\tname = row.find_element(By.CLASS_NAME, \"tupleNew__headingNrera\").text\n",
    "\texcept:\n",
    "\t\tname = np.nan\n",
    "\n",
    "\t# property location\n",
    "\ttry:\n",
    "\t\tlocation = row.find_element(By.CLASS_NAME, \"tupleNew__propType\").text\n",
    "\texcept:\n",
    "\t\tlocation = np.nan\n",
    "\n",
    "\t# property price\n",
    "\ttry:\n",
    "\t\tprice = row.find_element(By.CLASS_NAME, \"tupleNew__priceValWrap\").text\n",
    "\texcept:\n",
    "\t\tprice = np.nan\n",
    "\n",
    "\t# property area and bhk\n",
    "\ttry:\n",
    "\t\telements = row.find_elements(By.CLASS_NAME, \"tupleNew__area1Type\")\n",
    "\texcept:\n",
    "\t\tarea, bhk = [np.nan, np.nan]\n",
    "\telse:\n",
    "\t\tarea, bhk = [ele.text for ele in elements]\n",
    "\t\t\t\t\t\n",
    "\tproperty = {\n",
    "\t\t\"name\": name,\n",
    "\t\t\"location\": location,\n",
    "\t\t\"price\": price,\n",
    "\t\t\"area\": area,\n",
    "\t\t\"bhk\": bhk\n",
    "\t}\n",
    "\tdata.append(property)\n",
    "\n",
    "time.sleep(2)\n",
    "driver.quit()\n",
    "\n",
    "# ----- CLEANING THE DATA -----\n",
    "\n",
    "df_properties = (\n",
    "\tpd\n",
    "\t.DataFrame(data)\n",
    "\t.drop_duplicates()\n",
    "\t.apply(lambda col: col.str.strip().str.lower() if col.dtype == \"object\" else col)\n",
    "\t.assign(\n",
    "\t\tis_starred=lambda df_: df_.name.str.contains(\"\\n\").astype(int),\n",
    "\t\tname=lambda df_: (\n",
    "\t\t\tdf_\n",
    "\t\t\t.name\n",
    "\t\t\t.str.replace(\"\\n[0-9.]+\", \"\", regex=True)\n",
    "\t\t\t.str.strip()\n",
    "\t\t\t.replace(\"adroit district s\", \"adroit district's\")\n",
    "\t\t),\n",
    "\t\tlocation=lambda df_: (\n",
    "\t\t\tdf_\n",
    "\t\t\t.location\n",
    "\t\t\t.str.replace(\"chennai\", \"\")\n",
    "\t\t\t.str.strip()\n",
    "\t\t\t.str.replace(\",$\", \"\", regex=True)\n",
    "\t\t\t.str.split(\"in\")\n",
    "\t\t\t.str[-1]\n",
    "\t\t\t.str.strip()\n",
    "\t\t),\n",
    "\t\tprice=lambda df_: (\n",
    "\t\t\tdf_\n",
    "\t\t\t.price\n",
    "\t\t\t.str.replace(\"₹\", \"\")\n",
    "\t\t\t.apply(lambda val: float(val.replace(\"lac\", \"\").strip()) if \"lac\" in val else float(val.replace(\"cr\", \"\").strip()) * 100)\n",
    "\t\t),\n",
    "\t\tarea=lambda df_: (\n",
    "\t\t\tdf_\n",
    "\t\t\t.area\n",
    "\t\t\t.str.replace(\"sqft\", \"\")\n",
    "\t\t\t.str.strip()\n",
    "\t\t\t.str.replace(\",\", \"\")\n",
    "\t\t\t.pipe(lambda ser: pd.to_numeric(ser))\n",
    "\t\t),\n",
    "\t\tbhk=lambda df_: (\n",
    "\t\t\tdf_\n",
    "\t\t\t.bhk\n",
    "\t\t\t.str.replace(\"bhk\", \"\")\n",
    "\t\t\t.str.strip()\n",
    "\t\t\t.pipe(lambda ser: pd.to_numeric(ser))\n",
    "\t\t)\n",
    "\t)\n",
    "\t.rename(columns={\n",
    "\t\t\"price\": \"price_lakhs\",\n",
    "\t\t\"area\": \"area_sqft\"\n",
    "\t})\n",
    "\t.reset_index(drop=True)\n",
    "\t.to_excel(\"chennai-properties-99acres.xlsx\", index=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c09932c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The webpage \"India Real Estate Property Site - Buy Sell Rent Properties Portal - 99acres.com\" did get fully laoded.\n",
      "\n",
      "The webpage \"Property in Chennai - Real Estate in Chennai\" did get fully laoded.\n",
      "\n",
      "Timeout because we have uncovered all filters.\n",
      "\n",
      "Timeout while clicking on \"Next Page\".\n",
      "\n",
      "We have scraped 2 pages.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "class ProprertyScraper:\n",
    "\tdef __init__(self, url, timeout=5):\n",
    "\t\tself.url = url\n",
    "\t\tself.data = []\n",
    "\t\tself.driver = self._initialize_driver()\n",
    "\t\tself.wait = WebDriverWait(self.driver, timeout=timeout)\n",
    "\n",
    "\n",
    "\tdef _initialize_driver(self):\n",
    "\t\tchrome_options = Options()\n",
    "\t\tchrome_options.add_argument(\"--disable-http2\")\n",
    "\t\tchrome_options.add_argument(\"--incognito\")\n",
    "\t\tchrome_options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "\t\tchrome_options.add_argument(\"--ignore-certificate-errors\")\n",
    "\t\tchrome_options.add_argument(\"--enable-features=NetworkServiceInProcess\")\n",
    "\t\tchrome_options.add_argument(\"--disable-features=NetworkService\")\n",
    "\t\tchrome_options.add_argument(\n",
    "\t\t    \"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.63 Safari/537.36\"\n",
    "\t\t)\n",
    "\t\tdriver = webdriver.Chrome(options=chrome_options)\n",
    "\t\tdriver.maximize_window()\n",
    "\t\treturn driver\n",
    "\n",
    "\n",
    "\tdef _wait_for_page_to_load(self):\n",
    "\t\ttitle = self.driver.title\n",
    "\t\ttry:\n",
    "\t\t\tself.wait.until(\n",
    "\t\t\t\tlambda d: d.execute_script(\"return document.readyState\") == \"complete\"\n",
    "\t\t\t)\n",
    "\t\texcept:\n",
    "\t\t\tprint(f\"The webpage \\\"{title}\\\" did not get fully laoded.\\n\")\n",
    "\t\telse:\n",
    "\t\t\tprint(f\"The webpage \\\"{title}\\\" did get fully laoded.\\n\")\n",
    "\n",
    "\t\n",
    "\tdef access_website(self):\n",
    "\t\tself.driver.get(self.url)\n",
    "\t\tself._wait_for_page_to_load()\n",
    "\n",
    "\n",
    "\tdef search_properties(self, text):\n",
    "\t\t# locating and entering text in search bar\n",
    "\t\ttry:\n",
    "\t\t\tsearch_bar = self.wait.until(\n",
    "\t\t\t\tEC.presence_of_element_located((By.XPATH, '//*[@id=\"keyword2\"]'))\n",
    "\t\t\t)\n",
    "\t\texcept:\n",
    "\t\t\tprint(\"Timeout while locating Search Bar.\\n\")\n",
    "\t\telse:\n",
    "\t\t\tsearch_bar.send_keys(text)\n",
    "\t\t\ttime.sleep(2)\n",
    "\t\t\n",
    "\t\t# selecting valid option from list\n",
    "\t\ttry:\n",
    "\t\t\tvalid_option = self.wait.until(\n",
    "\t\t\t\tEC.element_to_be_clickable((By.XPATH, '//*[@id=\"0\"]'))\n",
    "\t\t\t)\n",
    "\t\texcept:\n",
    "\t\t\tprint(\"Timeout while locating valid search option.\\n\")\n",
    "\t\telse:\n",
    "\t\t\tvalid_option.click()\n",
    "\t\t\ttime.sleep(2)\n",
    "\t\t\n",
    "\t\t# click on Search button\n",
    "\t\ttry:\n",
    "\t\t\tsearch_button = self.wait.until(\n",
    "\t\t\t\tEC.element_to_be_clickable((By.XPATH, '//*[@id=\"searchform_search_btn\"]'))\n",
    "\t\t\t)\n",
    "\t\texcept:\n",
    "\t\t\tprint(\"Timeout while clicking on \\\"Search\\\" button.\\n\")\n",
    "\t\telse:\n",
    "\t\t\tsearch_button.click()\n",
    "\t\t\tself._wait_for_page_to_load()\n",
    "\n",
    "\n",
    "\tdef adjust_budget_slider(self, offset):\n",
    "\t\ttry:\n",
    "\t\t\tslider = self.wait.until(\n",
    "\t\t\t\tEC.element_to_be_clickable((By.XPATH, '//*[@id=\"budgetLeftFilter_max_node\"]'))\n",
    "\t\t\t)\n",
    "\t\texcept:\n",
    "\t\t\tprint(\"Timeout while clicking on Budget slider circle.\\n\")\n",
    "\t\telse:\n",
    "\t\t\tactions = ActionChains(self.driver)\n",
    "\t\t\t(\n",
    "\t\t\t\tactions\n",
    "\t\t\t\t.click_and_hold(slider)\n",
    "\t\t\t\t.move_by_offset(offset, 0)\n",
    "\t\t\t\t.release()\n",
    "\t\t\t\t.perform()\n",
    "\t\t\t)\n",
    "\t\t\ttime.sleep(2)\n",
    "\n",
    "\n",
    "\tdef apply_filters(self):\n",
    "\t\t# 1. Verified\n",
    "\t\tverified = self.wait.until(\n",
    "\t\t\tEC.element_to_be_clickable((By.XPATH, '/html[1]/body[1]/div[1]/div[1]/div[1]/div[4]/div[3]/div[1]/div[3]/section[1]/div[1]/div[1]/div[1]/div[1]/div[1]/div[1]/div[3]/span[2]'))\n",
    "\t\t)\n",
    "\t\tverified.click()\n",
    "\t\ttime.sleep(1)\n",
    "\t\t\n",
    "\t\t# 2. Ready To Move\n",
    "\t\tready_to_move = self.wait.until(\n",
    "\t\t\tEC.element_to_be_clickable((By.XPATH, '/html[1]/body[1]/div[1]/div[1]/div[1]/div[4]/div[3]/div[1]/div[3]/section[1]/div[1]/div[1]/div[1]/div[1]/div[1]/div[1]/div[5]/span[2]'))\n",
    "\t\t)\n",
    "\t\tready_to_move.click()\n",
    "\t\ttime.sleep(1)\n",
    "\t\t\n",
    "\t\t# moving to the right side to unhide remaining filters\n",
    "\t\twhile True:\n",
    "\t\t\ttry:\n",
    "\t\t\t\tfilter_right_button = self.wait.until(\n",
    "\t\t\t\t\tEC.presence_of_element_located((By.XPATH, \"//i[contains(@class,'iconS_Common_24 icon_upArrow cc__rightArrow')]\"))\n",
    "\t\t\t\t)\n",
    "\t\t\texcept:\n",
    "\t\t\t\tprint(\"Timeout because we have uncovered all filters.\\n\")\n",
    "\t\t\t\tbreak\n",
    "\t\t\telse:\n",
    "\t\t\t\tfilter_right_button.click()\n",
    "\t\t\t\ttime.sleep(10)\n",
    "\t\t\n",
    "\t\t# 3. With Photos\n",
    "\t\twith_photos = self.wait.until(\n",
    "\t\t\tEC.element_to_be_clickable((By.XPATH, '/html[1]/body[1]/div[1]/div[1]/div[1]/div[4]/div[3]/div[1]/div[3]/section[1]/div[1]/div[1]/div[1]/div[1]/div[2]/div[1]/div[6]/span[2]'))\n",
    "\t\t)\n",
    "\t\twith_photos.click()\n",
    "\t\ttime.sleep(10)\n",
    "\t\t\n",
    "\t\t# 4. With Videos\n",
    "\t\twith_videos = self.wait.until(\n",
    "\t\t\tEC.element_to_be_clickable((By.XPATH, '/html[1]/body[1]/div[1]/div[1]/div[1]/div[4]/div[3]/div[1]/div[3]/section[1]/div[1]/div[1]/div[1]/div[1]/div[2]/div[1]/div[7]/span[2]'))\n",
    "\t\t)\n",
    "\t\twith_videos.click()\n",
    "\t\ttime.sleep(10)\n",
    "\n",
    "\n",
    "\tdef _extract_data(self, row, by, value):\n",
    "\t\ttry:\n",
    "\t\t\treturn row.find_element(by, value).text\n",
    "\t\texcept:\n",
    "\t\t\treturn np.nan\n",
    "\t\n",
    "\n",
    "\tdef scrape_webpage(self):\n",
    "\t\trows = self.driver.find_elements(By.CLASS_NAME, \"tupleNew__TupleContent\")\n",
    "\t\tfor row in rows:\n",
    "\t\t\tproperty = {\n",
    "\t\t\t\t\"name\": self._extract_data(row, By.CLASS_NAME, \"tupleNew__headingNrera\"),\n",
    "\t\t\t\t\"location\": self._extract_data(row, By.CLASS_NAME, \"tupleNew__propType\"),\n",
    "\t\t\t\t\"price\": self._extract_data(row, By.CLASS_NAME, \"tupleNew__priceValWrap\")\n",
    "\t\t\t}\n",
    "\t\t\n",
    "\t\t\ttry:\n",
    "\t\t\t\telements = row.find_elements(By.CLASS_NAME, \"tupleNew__area1Type\")\n",
    "\t\t\texcept:\n",
    "\t\t\t\tproperty[\"area\"], property[\"bhk\"] = [np.nan, np.nan]\n",
    "\t\t\telse:\n",
    "\t\t\t\tproperty[\"area\"], property[\"bhk\"] = [ele.text for ele in elements]\n",
    "\t\t\t\t\n",
    "\t\t\tself.data.append(property)\n",
    "\t\t\n",
    "\n",
    "\tdef navigate_pages_and_scrape_data(self):\n",
    "\t\tpage_count = 0\n",
    "\t\twhile True:\n",
    "\t\t\tpage_count += 1\n",
    "\t\t\ttry:\n",
    "\t\t\t\tself.scrape_webpage()\n",
    "\t\t\t\tnext_page_button = self.driver.find_element(By.XPATH, \"//a[normalize-space()='Next Page >']\")\n",
    "\t\t\texcept:\n",
    "\t\t\t\tprint(f\"We have scraped {page_count} pages.\\n\")\n",
    "\t\t\t\tbreak\n",
    "\t\t\telse:\n",
    "\t\t\t\ttry:\n",
    "\t\t\t\t\tself.driver.execute_script(\"window.scrollBy(0, arguments[0].getBoundingClientRect().top - 100);\", next_page_button)\n",
    "\t\t\t\t\ttime.sleep(2)\t\t\t\n",
    "\t\t\t\t\tself.wait.until(\n",
    "\t\t\t\t\t\tEC.element_to_be_clickable((By.XPATH, \"//a[normalize-space()='Next Page >']\"))\n",
    "\t\t\t\t\t).click()\n",
    "\t\t\t\t\ttime.sleep(10)\n",
    "\t\t\t\texcept:\n",
    "\t\t\t\t\tprint(\"Timeout while clicking on \\\"Next Page\\\".\\n\")\n",
    "\n",
    "\n",
    "\tdef clean_data_and_save_as_excel(self, file_name):\n",
    "\t\tdf_properties = (\n",
    "\t\t\tpd\n",
    "\t\t\t.DataFrame(self.data)\n",
    "\t\t\t.drop_duplicates()\n",
    "\t\t\t.apply(lambda col: col.str.strip().str.lower() if col.dtype == \"object\" else col)\n",
    "\t\t\t.assign(\n",
    "\t\t\t\tis_starred=lambda df_: df_.name.str.contains(\"\\n\").astype(int),\n",
    "\t\t\t\tname=lambda df_: (\n",
    "\t\t\t\t\tdf_\n",
    "\t\t\t\t\t.name\n",
    "\t\t\t\t\t.str.replace(\"\\n[0-9.]+\", \"\", regex=True)\n",
    "\t\t\t\t\t.str.strip()\n",
    "\t\t\t\t\t.replace(\"adroit district s\", \"adroit district's\")\n",
    "\t\t\t\t),\n",
    "\t\t\t\tlocation=lambda df_: (\n",
    "\t\t\t\t\tdf_\n",
    "\t\t\t\t\t.location\n",
    "\t\t\t\t\t.str.replace(\"chennai\", \"\")\n",
    "\t\t\t\t\t.str.strip()\n",
    "\t\t\t\t\t.str.replace(\",$\", \"\", regex=True)\n",
    "\t\t\t\t\t.str.split(\"in\")\n",
    "\t\t\t\t\t.str[-1]\n",
    "\t\t\t\t\t.str.strip()\n",
    "\t\t\t\t),\n",
    "\t\t\t\tprice=lambda df_: (\n",
    "\t\t\t\t\tdf_\n",
    "\t\t\t\t\t.price\n",
    "\t\t\t\t\t.str.replace(\"₹\", \"\")\n",
    "\t\t\t\t\t.apply(lambda val: float(val.replace(\"lac\", \"\").strip()) if \"lac\" in val else float(val.replace(\"cr\", \"\").strip()) * 100)\n",
    "\t\t\t\t),\n",
    "\t\t\t\tarea=lambda df_: (\n",
    "\t\t\t\t\tdf_\n",
    "\t\t\t\t\t.area\n",
    "\t\t\t\t\t.str.replace(\"sqft\", \"\")\n",
    "\t\t\t\t\t.str.strip()\n",
    "\t\t\t\t\t.str.replace(\",\", \"\")\n",
    "\t\t\t\t\t.pipe(lambda ser: pd.to_numeric(ser))\n",
    "\t\t\t\t),\n",
    "\t\t\t\tbhk=lambda df_: (\n",
    "\t\t\t\t\tdf_\n",
    "\t\t\t\t\t.bhk\n",
    "\t\t\t\t\t.str.replace(\"bhk\", \"\")\n",
    "\t\t\t\t\t.str.strip()\n",
    "\t\t\t\t\t.pipe(lambda ser: pd.to_numeric(ser))\n",
    "\t\t\t\t)\n",
    "\t\t\t)\n",
    "\t\t\t.rename(columns={\n",
    "\t\t\t\t\"price\": \"price_lakhs\",\n",
    "\t\t\t\t\"area\": \"area_sqft\"\n",
    "\t\t\t})\n",
    "\t\t\t.reset_index(drop=True)\n",
    "\t\t)\n",
    "\t\tdf_properties.to_excel(f\"{file_name}.xlsx\", index=False)\n",
    "\n",
    "\t\n",
    "\tdef run(self, text=\"Chennai\", offset=-100, file_name=\"properties\"):\n",
    "\t\ttry:\n",
    "\t\t\tself.access_website()\n",
    "\t\t\tself.search_properties(text)\n",
    "\t\t\tself.adjust_budget_slider(offset)\n",
    "\t\t\tself.apply_filters()\n",
    "\t\t\tself.navigate_pages_and_scrape_data()\n",
    "\t\t\tself.clean_data_and_save_as_excel(file_name)\n",
    "\t\tfinally:\n",
    "\t\t\ttime.sleep(5)\n",
    "\t\t\tself.driver.quit()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\tscraper = ProprertyScraper(url=\"https://www.99acres.com/\")\n",
    "\tscraper.run(\n",
    "\t\ttext=\"chennai\",\n",
    "\t\toffset=-73,\n",
    "\t\tfile_name=\"chennai-properties\"\n",
    "\t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f635e2e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The webpage \"India Real Estate Property Site - Buy Sell Rent Properties Portal - 99acres.com\" loaded successfully.\n",
      "\n",
      "Could not click the first search suggestion. Trying alternative approach.\n",
      "The webpage \"Property in Chennai - Real Estate in Chennai\" loaded successfully.\n",
      "\n",
      "Successfully navigated to Chennai properties page.\n",
      "Budget filter applied successfully.\n",
      "Could not apply filter: Verified\n",
      "Still couldn't apply filter: Verified\n",
      "Could not apply filter: Ready To Move\n",
      "Still couldn't apply filter: Ready To Move\n",
      "Could not apply filter: With Photos\n",
      "Still couldn't apply filter: With Photos\n",
      "The webpage \"Property in Chennai - Real Estate in Chennai\" loaded successfully.\n",
      "\n",
      "All filters applied, proceeding to scrape data.\n",
      "Processing page 1...\n",
      "Found 1 properties on page 1\n",
      "Error navigating to next page: Message: stale element reference: stale element not found in the current frame\n",
      "  (Session info: chrome=135.0.7049.85); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#stale-element-reference-exception\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF7517E5335+78597]\n",
      "\tGetHandleVerifier [0x00007FF7517E5390+78688]\n",
      "\t(No symbol) [0x00007FF7515991AA]\n",
      "\t(No symbol) [0x00007FF7515A0ABC]\n",
      "\t(No symbol) [0x00007FF7515A3B1C]\n",
      "\t(No symbol) [0x00007FF7515A3BEF]\n",
      "\t(No symbol) [0x00007FF7515F130D]\n",
      "\t(No symbol) [0x00007FF7515E2170]\n",
      "\t(No symbol) [0x00007FF7516170EA]\n",
      "\t(No symbol) [0x00007FF7515E1AB6]\n",
      "\t(No symbol) [0x00007FF751617300]\n",
      "\t(No symbol) [0x00007FF75163F2BB]\n",
      "\t(No symbol) [0x00007FF751616EC3]\n",
      "\t(No symbol) [0x00007FF7515E03F8]\n",
      "\t(No symbol) [0x00007FF7515E1163]\n",
      "\tGetHandleVerifier [0x00007FF751A8EEED+2870973]\n",
      "\tGetHandleVerifier [0x00007FF751A89698+2848360]\n",
      "\tGetHandleVerifier [0x00007FF751AA6973+2967875]\n",
      "\tGetHandleVerifier [0x00007FF75180017A+188746]\n",
      "\tGetHandleVerifier [0x00007FF75180845F+222255]\n",
      "\tGetHandleVerifier [0x00007FF7517ED2B4+111236]\n",
      "\tGetHandleVerifier [0x00007FF7517ED462+111666]\n",
      "\tGetHandleVerifier [0x00007FF7517D3589+5465]\n",
      "\tBaseThreadInitThunk [0x00007FFFDC61E8D7+23]\n",
      "\tRtlUserThreadStart [0x00007FFFDD5714FC+44]\n",
      "\n",
      "Completed scraping 1 pages.\n",
      "Scraped 1 properties from 1 pages.\n",
      "Data saved to chennai-properties-99acres.xlsx with 1 records.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "\n",
    "# ----- SCRAPING THE DATA -----\n",
    "\n",
    "def wait_for_page_to_load(driver, wait_time=10):\n",
    "    \"\"\"Wait for page to load completely\"\"\"\n",
    "    title = driver.title\n",
    "    wait = WebDriverWait(driver, wait_time)\n",
    "    try:\n",
    "        wait.until(lambda d: d.execute_script(\"return document.readyState\") == \"complete\")\n",
    "        print(f\"The webpage \\\"{title}\\\" loaded successfully.\\n\")\n",
    "        # Additional wait for any AJAX content to load\n",
    "        time.sleep(2)\n",
    "    except:\n",
    "        print(f\"Warning: The webpage \\\"{title}\\\" may not have fully loaded.\\n\")\n",
    "\n",
    "def scroll_to_element(driver, element):\n",
    "    \"\"\"Scroll to make element visible\"\"\"\n",
    "    driver.execute_script(\"arguments[0].scrollIntoView({block: 'center'});\", element)\n",
    "    time.sleep(1)\n",
    "\n",
    "def extract_property_data(row):\n",
    "    \"\"\"Extract data from a property row\"\"\"\n",
    "    property_data = {}\n",
    "    \n",
    "    # Property name\n",
    "    try:\n",
    "        property_data[\"name\"] = row.find_element(By.CSS_SELECTOR, \".tupleNew__headingNrera, .srpTuple__propertyName\").text\n",
    "    except:\n",
    "        property_data[\"name\"] = np.nan\n",
    "\n",
    "    # Property location\n",
    "    try:\n",
    "        property_data[\"location\"] = row.find_element(By.CSS_SELECTOR, \".tupleNew__propType, .srpTuple__locationName\").text\n",
    "    except:\n",
    "        property_data[\"location\"] = np.nan\n",
    "\n",
    "    # Property price\n",
    "    try:\n",
    "        property_data[\"price\"] = row.find_element(By.CSS_SELECTOR, \".tupleNew__priceValWrap, .srpTuple__price\").text\n",
    "    except:\n",
    "        property_data[\"price\"] = np.nan\n",
    "\n",
    "    # Property area and bhk\n",
    "    try:\n",
    "        elements = row.find_elements(By.CSS_SELECTOR, \".tupleNew__area1Type, .srpTuple__dFlex.srpTuple__srpDataWrap span:nth-child(1), .srpTuple__dFlex.srpTuple__srpDataWrap span:nth-child(3)\")\n",
    "        if len(elements) >= 2:\n",
    "            property_data[\"area\"] = elements[0].text\n",
    "            property_data[\"bhk\"] = elements[1].text\n",
    "        else:\n",
    "            property_data[\"area\"] = elements[0].text if elements else np.nan\n",
    "            property_data[\"bhk\"] = np.nan\n",
    "    except:\n",
    "        property_data[\"area\"] = np.nan\n",
    "        property_data[\"bhk\"] = np.nan\n",
    "        \n",
    "    return property_data\n",
    "\n",
    "# Configure Chrome options\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--disable-http2\")\n",
    "chrome_options.add_argument(\"--incognito\")\n",
    "chrome_options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "chrome_options.add_argument(\"--ignore-certificate-errors\")\n",
    "chrome_options.add_argument(\"--disable-notifications\")  # Disable notifications\n",
    "chrome_options.add_argument(\"--disable-popup-blocking\")  # Disable popup blocks\n",
    "chrome_options.add_argument(\n",
    "    \"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.6261.112 Safari/537.36\"\n",
    ")\n",
    "\n",
    "# Initialize driver\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "driver.maximize_window()\n",
    "\n",
    "# Create WebDriverWait instance with increased timeout\n",
    "wait = WebDriverWait(driver, 15)  # Increased timeout to 15 seconds\n",
    "\n",
    "# Track our data\n",
    "data = []\n",
    "page_count = 0\n",
    "total_properties = 0\n",
    "\n",
    "try:\n",
    "    # Access the target webpage\n",
    "    url = \"https://www.99acres.com/\"\n",
    "    driver.get(url)\n",
    "    wait_for_page_to_load(driver)\n",
    "\n",
    "    # Search for Chennai properties\n",
    "    try:\n",
    "        search_bar = wait.until(EC.presence_of_element_located((By.ID, \"keyword2\")))\n",
    "        search_bar.clear()\n",
    "        search_bar.send_keys(\"Chennai\")\n",
    "        time.sleep(2)\n",
    "        \n",
    "        # Select the first suggestion\n",
    "        try:\n",
    "            valid_option = wait.until(EC.element_to_be_clickable((By.XPATH, \"//div[@class='autocomplete_drop']//li[1]\")))\n",
    "            valid_option.click()\n",
    "        except:\n",
    "            print(\"Could not click the first search suggestion. Trying alternative approach.\")\n",
    "            search_bar.send_keys(Keys.DOWN)\n",
    "            search_bar.send_keys(Keys.ENTER)\n",
    "        \n",
    "        time.sleep(1)\n",
    "        \n",
    "        # Click search button\n",
    "        search_button = wait.until(EC.element_to_be_clickable((By.ID, \"searchform_search_btn\")))\n",
    "        search_button.click()\n",
    "        wait_for_page_to_load(driver)\n",
    "        \n",
    "        print(\"Successfully navigated to Chennai properties page.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during search process: {e}\")\n",
    "    \n",
    "    # Apply filters more robustly\n",
    "    try:\n",
    "        # Set max budget using slider\n",
    "        try:\n",
    "            slider = wait.until(EC.presence_of_element_located((By.XPATH, '//*[contains(@id,\"budgetLeftFilter_max_node\")]')))\n",
    "            scroll_to_element(driver, slider)\n",
    "            actions = ActionChains(driver)\n",
    "            actions.click_and_hold(slider).move_by_offset(-73, 0).release().perform()\n",
    "            time.sleep(2)\n",
    "            print(\"Budget filter applied successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Budget slider adjustment failed: {e}\")\n",
    "        \n",
    "        # Apply Verified, Ready To Move, With Photos filters\n",
    "        filter_options = [\n",
    "            {\"name\": \"Verified\", \"xpath\": \"//div[contains(@class,'srpFilterWrapper')]//span[contains(text(),'Verified')]/..\"},\n",
    "            {\"name\": \"Ready To Move\", \"xpath\": \"//div[contains(@class,'srpFilterWrapper')]//span[contains(text(),'Ready To Move')]/..\"},\n",
    "            {\"name\": \"With Photos\", \"xpath\": \"//div[contains(@class,'srpFilterWrapper')]//span[contains(text(),'With Photos')]/..\"}\n",
    "        ]\n",
    "        \n",
    "        for filter_opt in filter_options:\n",
    "            try:\n",
    "                # Try to find the filter button\n",
    "                filter_element = driver.find_element(By.XPATH, filter_opt[\"xpath\"])\n",
    "                scroll_to_element(driver, filter_element)\n",
    "                filter_element.click()\n",
    "                print(f\"Applied filter: {filter_opt['name']}\")\n",
    "                time.sleep(2)\n",
    "            except:\n",
    "                print(f\"Could not apply filter: {filter_opt['name']}\")\n",
    "                \n",
    "                # Try to navigate right to see more filters if needed\n",
    "                try:\n",
    "                    right_arrow = driver.find_element(By.XPATH, \"//i[contains(@class,'rightArrow')]\")\n",
    "                    scroll_to_element(driver, right_arrow)\n",
    "                    right_arrow.click()\n",
    "                    time.sleep(2)\n",
    "                    \n",
    "                    # Try again to apply the filter\n",
    "                    filter_element = driver.find_element(By.XPATH, filter_opt[\"xpath\"])\n",
    "                    scroll_to_element(driver, filter_element)\n",
    "                    filter_element.click()\n",
    "                    print(f\"Applied filter: {filter_opt['name']} after scrolling\")\n",
    "                    time.sleep(2)\n",
    "                except:\n",
    "                    print(f\"Still couldn't apply filter: {filter_opt['name']}\")\n",
    "        \n",
    "        # Wait for filters to apply and results to load\n",
    "        time.sleep(5)\n",
    "        wait_for_page_to_load(driver)\n",
    "        print(\"All filters applied, proceeding to scrape data.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error applying filters: {e}\")\n",
    "    \n",
    "    # Process all pages\n",
    "    while True:\n",
    "        page_count += 1\n",
    "        print(f\"Processing page {page_count}...\")\n",
    "        \n",
    "        # Wait for property listings to load\n",
    "        try:\n",
    "            wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \".srpTuple__tupleDetails, .tupleNew__TupleContent\")))\n",
    "        except TimeoutException:\n",
    "            print(\"Could not find property listings on this page.\")\n",
    "        \n",
    "        # Scroll down the page to ensure all content loads\n",
    "        driver.execute_script(\"window.scrollTo(0, 0);\")  # Start at top\n",
    "        last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        while True:\n",
    "            driver.execute_script(\"window.scrollBy(0, 500);\")\n",
    "            time.sleep(0.5)\n",
    "            new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            if new_height == last_height:\n",
    "                break\n",
    "            last_height = new_height\n",
    "        \n",
    "        # Extract data from all property listings on current page\n",
    "        property_rows = driver.find_elements(By.CSS_SELECTOR, \".srpTuple__tupleDetails, .tupleNew__TupleContent\")\n",
    "        print(f\"Found {len(property_rows)} properties on page {page_count}\")\n",
    "        \n",
    "        for row in property_rows:\n",
    "            try:\n",
    "                property_data = extract_property_data(row)\n",
    "                data.append(property_data)\n",
    "                total_properties += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Error extracting property data: {e}\")\n",
    "        \n",
    "        # Check if next page exists and navigate to it\n",
    "        try:\n",
    "            # Find the next page button by multiple possible selectors\n",
    "            next_page_selectors = [\n",
    "                \"//a[contains(text(),'Next') or contains(text(),'next')]\",\n",
    "                \"//a[contains(@class,'pgdiv')][contains(text(),'Next')]\",\n",
    "                \"//div[contains(@class,'pagination')]//a[contains(text(),'Next')]\",\n",
    "                \"//a[normalize-space()='Next Page >']\"\n",
    "            ]\n",
    "            \n",
    "            next_page_button = None\n",
    "            for selector in next_page_selectors:\n",
    "                try:\n",
    "                    next_page_button = driver.find_element(By.XPATH, selector)\n",
    "                    break\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            if not next_page_button:\n",
    "                print(f\"No next page button found. Scraped {page_count} pages in total.\")\n",
    "                break\n",
    "                \n",
    "            # Check if \"Next\" is actually clickable (not disabled)\n",
    "            if \"disabled\" in next_page_button.get_attribute(\"class\"):\n",
    "                print(f\"Next page button is disabled. Scraped {page_count} pages in total.\")\n",
    "                break\n",
    "                \n",
    "            # Scroll to the next page button\n",
    "            scroll_to_element(driver, next_page_button)\n",
    "            \n",
    "            # Click the next page button\n",
    "            next_page_button.click()\n",
    "            print(f\"Navigated to page {page_count + 1}\")\n",
    "            \n",
    "            # Wait for the new page to load\n",
    "            time.sleep(5)\n",
    "            wait_for_page_to_load(driver)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error navigating to next page: {e}\")\n",
    "            print(f\"Completed scraping {page_count} pages.\")\n",
    "            break\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Unexpected error: {e}\")\n",
    "finally:\n",
    "    print(f\"Scraped {total_properties} properties from {page_count} pages.\")\n",
    "    driver.quit()\n",
    "\n",
    "# ----- CLEANING THE DATA -----\n",
    "\n",
    "if data:\n",
    "    df_properties = pd.DataFrame(data)\n",
    "    \n",
    "    # Clean and transform the data\n",
    "    df_properties = (\n",
    "        df_properties\n",
    "        .drop_duplicates()\n",
    "        .apply(lambda col: col.str.strip().str.lower() if isinstance(col, pd.Series) and col.dtype == \"object\" else col)\n",
    "        .assign(\n",
    "            is_starred=lambda df_: df_.name.str.contains(\"\\n\", na=False).astype(int),\n",
    "            name=lambda df_: (\n",
    "                df_\n",
    "                .name\n",
    "                .str.replace(\"\\n[0-9.]+\", \"\", regex=True)\n",
    "                .str.strip()\n",
    "                .replace(\"adroit district s\", \"adroit district's\")\n",
    "            ),\n",
    "            location=lambda df_: (\n",
    "                df_\n",
    "                .location\n",
    "                .str.replace(\"chennai\", \"\", regex=False)\n",
    "                .str.strip()\n",
    "                .str.replace(\",$\", \"\", regex=True)\n",
    "                .str.split(\"in\")\n",
    "                .str[-1]\n",
    "                .str.strip()\n",
    "            ),\n",
    "            price=lambda df_: (\n",
    "                df_\n",
    "                .price\n",
    "                .str.replace(\"₹\", \"\", regex=False)\n",
    "                .apply(lambda val: float(val.replace(\"lac\", \"\").strip()) if isinstance(val, str) and \"lac\" in val \n",
    "                      else float(val.replace(\"cr\", \"\").strip()) * 100 if isinstance(val, str) and \"cr\" in val\n",
    "                      else np.nan)\n",
    "            ),\n",
    "            area=lambda df_: (\n",
    "                df_\n",
    "                .area\n",
    "                .str.replace(\"sqft\", \"\", regex=False)\n",
    "                .str.strip()\n",
    "                .str.replace(\",\", \"\", regex=False)\n",
    "                .pipe(lambda ser: pd.to_numeric(ser, errors='coerce'))\n",
    "            ),\n",
    "            bhk=lambda df_: (\n",
    "                df_\n",
    "                .bhk\n",
    "                .str.extract(r'(\\d+\\.?\\d*)', expand=False)\n",
    "                .pipe(lambda ser: pd.to_numeric(ser, errors='coerce'))\n",
    "            )\n",
    "        )\n",
    "        .rename(columns={\n",
    "            \"price\": \"price_lakhs\",\n",
    "            \"area\": \"area_sqft\"\n",
    "        })\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    \n",
    "    # Save to Excel\n",
    "    df_properties.to_excel(\"chennai-properties-99acres.xlsx\", index=False)\n",
    "    print(f\"Data saved to chennai-properties-99acres.xlsx with {len(df_properties)} records.\")\n",
    "else:\n",
    "    print(\"No data was collected to save.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9fc1f36d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The webpage \"India Real Estate Property Site - Buy Sell Rent Properties Portal - 99acres.com\" loaded successfully.\n",
      "\n",
      "Could not click the first search suggestion. Trying alternative approach.\n",
      "The webpage \"Property in Chennai - Real Estate in Chennai\" loaded successfully.\n",
      "\n",
      "Successfully navigated to Chennai properties page.\n",
      "Budget filter applied successfully.\n",
      "Attempting to apply Verified filter...\n",
      "Applied filter: Verified\n",
      "Attempting to apply Ready To Move filter...\n",
      "Applied filter: Ready To Move\n",
      "Attempting to apply With Photos filter...\n",
      "Applied filter: With Photos\n",
      "Attempting to apply With Videos filter...\n",
      "Scrolled filter section right\n",
      "Applied filter: With Videos after scrolling\n",
      "The webpage \"Property in Chennai - Real Estate in Chennai\" loaded successfully.\n",
      "\n",
      "Navigating to page 1: https://www.99acres.com/search/property/buy/chennai?city=32&preference=S&area_unit=1&res_com=R&page=1\n",
      "The webpage \"Page 1 - Property in Chennai - Real Estate in Chennai\" loaded successfully.\n",
      "\n",
      "Unexpected error: name 'document_height' is not defined\n",
      "Scraped 0 properties from 1 pages.\n",
      "No data was collected to save.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException, StaleElementReferenceException\n",
    "\n",
    "# ----- HELPER FUNCTIONS -----\n",
    "\n",
    "def wait_for_page_to_load(driver, wait_time=15):\n",
    "    \"\"\"Wait for page to load completely\"\"\"\n",
    "    title = driver.title\n",
    "    wait = WebDriverWait(driver, wait_time)\n",
    "    try:\n",
    "        wait.until(lambda d: d.execute_script(\"return document.readyState\") == \"complete\")\n",
    "        print(f\"The webpage \\\"{title}\\\" loaded successfully.\\n\")\n",
    "        # Additional wait for any AJAX content to load\n",
    "        time.sleep(3)\n",
    "    except:\n",
    "        print(f\"Warning: The webpage \\\"{title}\\\" may not have fully loaded.\\n\")\n",
    "\n",
    "def scroll_to_element(driver, element):\n",
    "    \"\"\"Scroll to make element visible\"\"\"\n",
    "    try:\n",
    "        driver.execute_script(\"arguments[0].scrollIntoView({block: 'center', behavior: 'smooth'});\", element)\n",
    "        time.sleep(2)\n",
    "    except:\n",
    "        print(\"Could not scroll to element\")\n",
    "\n",
    "def extract_property_data(row):\n",
    "    \"\"\"Extract data from a property row\"\"\"\n",
    "    property_data = {}\n",
    "    \n",
    "    # Property name\n",
    "    try:\n",
    "        property_data[\"name\"] = row.find_element(By.CSS_SELECTOR, \".tupleNew__headingNrera, .srpTuple__propertyName\").text\n",
    "    except:\n",
    "        property_data[\"name\"] = np.nan\n",
    "\n",
    "    # Property location\n",
    "    try:\n",
    "        property_data[\"location\"] = row.find_element(By.CSS_SELECTOR, \".tupleNew__propType, .srpTuple__locationName\").text\n",
    "    except:\n",
    "        property_data[\"location\"] = np.nan\n",
    "\n",
    "    # Property price\n",
    "    try:\n",
    "        property_data[\"price\"] = row.find_element(By.CSS_SELECTOR, \".tupleNew__priceValWrap, .srpTuple__price\").text\n",
    "    except:\n",
    "        property_data[\"price\"] = np.nan\n",
    "\n",
    "    # Property area and bhk\n",
    "    try:\n",
    "        elements = row.find_elements(By.CSS_SELECTOR, \".tupleNew__area1Type, .srpTuple__dFlex.srpTuple__srpDataWrap span:nth-child(1), .srpTuple__dFlex.srpTuple__srpDataWrap span:nth-child(3)\")\n",
    "        if len(elements) >= 2:\n",
    "            property_data[\"area\"] = elements[0].text\n",
    "            property_data[\"bhk\"] = elements[1].text\n",
    "        else:\n",
    "            property_data[\"area\"] = elements[0].text if elements else np.nan\n",
    "            property_data[\"bhk\"] = np.nan\n",
    "    except:\n",
    "        property_data[\"area\"] = np.nan\n",
    "        property_data[\"bhk\"] = np.nan\n",
    "        \n",
    "    return property_data\n",
    "\n",
    "def safe_click(driver, element):\n",
    "    \"\"\"Attempt to safely click an element with multiple strategies\"\"\"\n",
    "    try:\n",
    "        # Try regular click\n",
    "        element.click()\n",
    "        return True\n",
    "    except:\n",
    "        try:\n",
    "            # Try JavaScript click\n",
    "            driver.execute_script(\"arguments[0].click();\", element)\n",
    "            return True\n",
    "        except:\n",
    "            try:\n",
    "                # Try ActionChains click\n",
    "                ActionChains(driver).move_to_element(element).click().perform()\n",
    "                return True\n",
    "            except:\n",
    "                return False\n",
    "\n",
    "def find_element_with_retry(driver, by, value, max_attempts=3):\n",
    "    \"\"\"Find an element with retry logic to handle stale references\"\"\"\n",
    "    for attempt in range(max_attempts):\n",
    "        try:\n",
    "            return driver.find_element(by, value)\n",
    "        except StaleElementReferenceException:\n",
    "            if attempt == max_attempts - 1:\n",
    "                raise\n",
    "            time.sleep(1)\n",
    "\n",
    "def find_elements_with_retry(driver, by, value, max_attempts=3):\n",
    "    \"\"\"Find elements with retry logic to handle stale references\"\"\"\n",
    "    for attempt in range(max_attempts):\n",
    "        try:\n",
    "            return driver.find_elements(by, value)\n",
    "        except StaleElementReferenceException:\n",
    "            if attempt == max_attempts - 1:\n",
    "                raise\n",
    "            time.sleep(1)\n",
    "\n",
    "# ----- MAIN SCRAPING FUNCTION -----\n",
    "\n",
    "def scrape_99acres():\n",
    "    # Configure Chrome options\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--disable-http2\")\n",
    "    chrome_options.add_argument(\"--incognito\")\n",
    "    chrome_options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    chrome_options.add_argument(\"--ignore-certificate-errors\")\n",
    "    chrome_options.add_argument(\"--disable-notifications\")\n",
    "    chrome_options.add_argument(\"--disable-popup-blocking\")\n",
    "    chrome_options.add_argument(\n",
    "        \"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.6261.112 Safari/537.36\"\n",
    "    )\n",
    "\n",
    "    # Initialize driver\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    driver.maximize_window()\n",
    "\n",
    "    # Create WebDriverWait instance with increased timeout\n",
    "    wait = WebDriverWait(driver, 20)  # Increased timeout to 20 seconds\n",
    "\n",
    "    # Track our data\n",
    "    data = []\n",
    "    page_count = 0\n",
    "    total_properties = 0\n",
    "\n",
    "    try:\n",
    "        # Access the target webpage\n",
    "        url = \"https://www.99acres.com/\"\n",
    "        driver.get(url)\n",
    "        wait_for_page_to_load(driver)\n",
    "\n",
    "        # Search for Chennai properties\n",
    "        try:\n",
    "            search_bar = wait.until(EC.presence_of_element_located((By.ID, \"keyword2\")))\n",
    "            search_bar.clear()\n",
    "            search_bar.send_keys(\"Chennai\")\n",
    "            time.sleep(2)\n",
    "            \n",
    "            # Select the first suggestion\n",
    "            try:\n",
    "                valid_option = wait.until(EC.element_to_be_clickable((By.XPATH, \"//div[@class='autocomplete_drop']//li[1]\")))\n",
    "                safe_click(driver, valid_option)\n",
    "            except:\n",
    "                print(\"Could not click the first search suggestion. Trying alternative approach.\")\n",
    "                search_bar.send_keys(Keys.DOWN)\n",
    "                search_bar.send_keys(Keys.ENTER)\n",
    "            \n",
    "            time.sleep(2)\n",
    "            \n",
    "            # Click search button\n",
    "            search_button = wait.until(EC.element_to_be_clickable((By.ID, \"searchform_search_btn\")))\n",
    "            safe_click(driver, search_button)\n",
    "            wait_for_page_to_load(driver)\n",
    "            \n",
    "            print(\"Successfully navigated to Chennai properties page.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error during search process: {e}\")\n",
    "        \n",
    "        # Apply filters\n",
    "        try:\n",
    "            # Wait for filters to load\n",
    "            time.sleep(5)\n",
    "            \n",
    "            # Try to apply Budget filter\n",
    "            try:\n",
    "                slider = wait.until(EC.presence_of_element_located((By.XPATH, '//*[contains(@id,\"budgetLeftFilter_max_node\")]')))\n",
    "                scroll_to_element(driver, slider)\n",
    "                actions = ActionChains(driver)\n",
    "                actions.click_and_hold(slider).move_by_offset(-73, 0).release().perform()\n",
    "                time.sleep(2)\n",
    "                print(\"Budget filter applied successfully.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Budget slider adjustment failed: {e}\")\n",
    "            \n",
    "            # Apply main filters\n",
    "            filters_to_apply = [\"Verified\", \"Ready To Move\", \"With Photos\", \"With Videos\"]\n",
    "            \n",
    "            for filter_name in filters_to_apply:\n",
    "                print(f\"Attempting to apply {filter_name} filter...\")\n",
    "                \n",
    "                # Try multiple XPath approaches to find the filter\n",
    "                xpath_patterns = [\n",
    "                    f\"//div[contains(@class,'srpFilterWrapper')]//span[contains(text(),'{filter_name}')]/parent::*\",\n",
    "                    f\"//div[contains(@class,'filterSection')]//span[contains(text(),'{filter_name}')]/parent::*\",\n",
    "                    f\"//span[text()='{filter_name}']/parent::*\",\n",
    "                    f\"//*[contains(text(),'{filter_name}') and not(ancestor::*[contains(@style,'display: none')])]\"\n",
    "                ]\n",
    "                \n",
    "                filter_found = False\n",
    "                for xpath in xpath_patterns:\n",
    "                    try:\n",
    "                        filter_elements = driver.find_elements(By.XPATH, xpath)\n",
    "                        for element in filter_elements:\n",
    "                            if element.is_displayed():\n",
    "                                scroll_to_element(driver, element)\n",
    "                                if safe_click(driver, element):\n",
    "                                    filter_found = True\n",
    "                                    print(f\"Applied filter: {filter_name}\")\n",
    "                                    time.sleep(2)\n",
    "                                    break\n",
    "                        if filter_found:\n",
    "                            break\n",
    "                    except Exception as e:\n",
    "                        continue\n",
    "                \n",
    "                # If filter not found, try to scroll right to see more filters\n",
    "                if not filter_found:\n",
    "                    try:\n",
    "                        right_arrows = driver.find_elements(By.XPATH, \"//i[contains(@class,'rightArrow')] | //span[contains(@class,'rightArrow')]\")\n",
    "                        for arrow in right_arrows:\n",
    "                            if arrow.is_displayed():\n",
    "                                scroll_to_element(driver, arrow)\n",
    "                                if safe_click(driver, arrow):\n",
    "                                    print(\"Scrolled filter section right\")\n",
    "                                    time.sleep(3)\n",
    "                                    \n",
    "                                    # Try filter again after scrolling\n",
    "                                    for xpath in xpath_patterns:\n",
    "                                        try:\n",
    "                                            filter_elements = driver.find_elements(By.XPATH, xpath)\n",
    "                                            for element in filter_elements:\n",
    "                                                if element.is_displayed():\n",
    "                                                    scroll_to_element(driver, element)\n",
    "                                                    if safe_click(driver, element):\n",
    "                                                        filter_found = True\n",
    "                                                        print(f\"Applied filter: {filter_name} after scrolling\")\n",
    "                                                        time.sleep(2)\n",
    "                                                        break\n",
    "                                            if filter_found:\n",
    "                                                break\n",
    "                                        except:\n",
    "                                            continue\n",
    "                                    break\n",
    "                    except Exception as e:\n",
    "                        print(f\"Could not scroll filter section: {e}\")\n",
    "                \n",
    "                if not filter_found:\n",
    "                    print(f\"Could not apply filter: {filter_name}\")\n",
    "            \n",
    "            # Wait for filters to apply\n",
    "            time.sleep(5)\n",
    "            wait_for_page_to_load(driver)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error applying filters: {e}\")\n",
    "        \n",
    "        # ===== PAGINATION STRATEGY: DIRECT URL MANIPULATION =====\n",
    "        # Get current URL to use as base for pagination\n",
    "        base_url = driver.current_url\n",
    "        \n",
    "        # Remove any existing page parameters\n",
    "        if \"page=\" in base_url:\n",
    "            base_url = base_url.split(\"page=\")[0]\n",
    "            if base_url.endswith(\"&\"):\n",
    "                page_url_template = f\"{base_url}page=\"\n",
    "            else:\n",
    "                page_url_template = f\"{base_url}&page=\"\n",
    "        else:\n",
    "            if \"?\" in base_url:\n",
    "                page_url_template = f\"{base_url}&page=\"\n",
    "            else:\n",
    "                page_url_template = f\"{base_url}?page=\"\n",
    "        \n",
    "        # Start with page 1\n",
    "        current_page = 1\n",
    "        max_pages_to_scrape = 20  # Limit to prevent infinite loops\n",
    "        consecutive_empty_pages = 0\n",
    "        \n",
    "        while current_page <= max_pages_to_scrape and consecutive_empty_pages < 3:\n",
    "            page_url = f\"{page_url_template}{current_page}\"\n",
    "            print(f\"Navigating to page {current_page}: {page_url}\")\n",
    "            \n",
    "            driver.get(page_url)\n",
    "            wait_for_page_to_load(driver)\n",
    "            page_count += 1\n",
    "            \n",
    "            # Scroll down the page gradually to ensure all content loads\n",
    "            driver.execute_script(\"window.scrollTo(0, 0);\")  # Start at top\n",
    "            for i in range(10):  # Scroll in 10 steps\n",
    "                driver.execute_script(f\"window.scrollTo(0, {(i+1) * document_height / 10});\")\n",
    "                time.sleep(0.5)\n",
    "            \n",
    "            # Get property listings\n",
    "            try:\n",
    "                property_rows = wait.until(EC.presence_of_all_elements_located((\n",
    "                    By.CSS_SELECTOR, \".srpTuple__tupleDetails, .tupleNew__TupleContent\"\n",
    "                )))\n",
    "                print(f\"Found {len(property_rows)} properties on page {current_page}\")\n",
    "                \n",
    "                if len(property_rows) == 0:\n",
    "                    consecutive_empty_pages += 1\n",
    "                    print(f\"Empty page detected. Consecutive empty pages: {consecutive_empty_pages}\")\n",
    "                else:\n",
    "                    consecutive_empty_pages = 0\n",
    "                \n",
    "                # Extract data\n",
    "                for row in property_rows:\n",
    "                    try:\n",
    "                        property_data = extract_property_data(row)\n",
    "                        data.append(property_data)\n",
    "                        total_properties += 1\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error extracting property data: {e}\")\n",
    "                        \n",
    "            except TimeoutException:\n",
    "                print(f\"No properties found on page {current_page}\")\n",
    "                consecutive_empty_pages += 1\n",
    "                print(f\"Consecutive empty pages: {consecutive_empty_pages}\")\n",
    "                \n",
    "            # Go to next page\n",
    "            current_page += 1\n",
    "            time.sleep(3)  # Wait before loading next page\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")\n",
    "    finally:\n",
    "        print(f\"Scraped {total_properties} properties from {page_count} pages.\")\n",
    "        driver.quit()\n",
    "        return data\n",
    "\n",
    "# ----- RUN THE SCRAPER -----\n",
    "\n",
    "data = scrape_99acres()\n",
    "\n",
    "# ----- CLEANING THE DATA -----\n",
    "\n",
    "if data:\n",
    "    df_properties = pd.DataFrame(data)\n",
    "    \n",
    "    # Clean and transform the data\n",
    "    df_properties = (\n",
    "        df_properties\n",
    "        .drop_duplicates()\n",
    "        .apply(lambda col: col.str.strip().str.lower() if isinstance(col, pd.Series) and col.dtype == \"object\" else col)\n",
    "    )\n",
    "    \n",
    "    # Process each column individually with error handling\n",
    "    try:\n",
    "        # Process name column\n",
    "        if 'name' in df_properties.columns:\n",
    "            df_properties['is_starred'] = df_properties['name'].str.contains(\"\\n\", na=False).astype(int)\n",
    "            df_properties['name'] = (\n",
    "                df_properties['name']\n",
    "                .str.replace(\"\\n[0-9.]+\", \"\", regex=True)\n",
    "                .str.strip()\n",
    "                .replace(\"adroit district s\", \"adroit district's\")\n",
    "            )\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing 'name' column: {e}\")\n",
    "        \n",
    "    try:\n",
    "        # Process location column\n",
    "        if 'location' in df_properties.columns:\n",
    "            df_properties['location'] = (\n",
    "                df_properties['location']\n",
    "                .str.replace(\"chennai\", \"\", regex=False)\n",
    "                .str.strip()\n",
    "                .str.replace(\",$\", \"\", regex=True)\n",
    "                .str.split(\"in\")\n",
    "                .str[-1]\n",
    "                .str.strip()\n",
    "            )\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing 'location' column: {e}\")\n",
    "        \n",
    "    try:\n",
    "        # Process price column\n",
    "        if 'price' in df_properties.columns:\n",
    "            def process_price(val):\n",
    "                if not isinstance(val, str):\n",
    "                    return np.nan\n",
    "                try:\n",
    "                    val = val.replace(\"₹\", \"\").strip()\n",
    "                    if \"lac\" in val:\n",
    "                        return float(val.replace(\"lac\", \"\").strip())\n",
    "                    elif \"cr\" in val:\n",
    "                        return float(val.replace(\"cr\", \"\").strip()) * 100\n",
    "                    else:\n",
    "                        return np.nan\n",
    "                except:\n",
    "                    return np.nan\n",
    "                    \n",
    "            df_properties['price_lakhs'] = df_properties['price'].apply(process_price)\n",
    "            df_properties = df_properties.drop('price', axis=1)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing 'price' column: {e}\")\n",
    "        \n",
    "    try:\n",
    "        # Process area column\n",
    "        if 'area' in df_properties.columns:\n",
    "            df_properties['area_sqft'] = (\n",
    "                df_properties['area']\n",
    "                .str.replace(\"sqft\", \"\", regex=False)\n",
    "                .str.strip()\n",
    "                .str.replace(\",\", \"\", regex=False)\n",
    "                .pipe(lambda ser: pd.to_numeric(ser, errors='coerce'))\n",
    "            )\n",
    "            df_properties = df_properties.drop('area', axis=1)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing 'area' column: {e}\")\n",
    "        \n",
    "    try:\n",
    "        # Process bhk column\n",
    "        if 'bhk' in df_properties.columns:\n",
    "            df_properties['bhk'] = (\n",
    "                df_properties['bhk']\n",
    "                .str.extract(r'(\\d+\\.?\\d*)', expand=False)\n",
    "                .pipe(lambda ser: pd.to_numeric(ser, errors='coerce'))\n",
    "            )\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing 'bhk' column: {e}\")\n",
    "    \n",
    "    # Reset index and save to Excel\n",
    "    df_properties = df_properties.reset_index(drop=True)\n",
    "    df_properties.to_excel(\"chennai-properties-99acres.xlsx\", index=False)\n",
    "    print(f\"Data saved to chennai-properties-99acres.xlsx with {len(df_properties)} records.\")\n",
    "else:\n",
    "    print(\"No data was collected to save.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6236dc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "  \"data\": {\n",
    "    \"loan_analysis\": {\n",
    "      \"__typename\": \"LoanAnalysis\",\n",
    "      \"market\": {\n",
    "        \"__typename\": \"Market\",\n",
    "        \"mortgage_data\": {\n",
    "          \"__typename\": \"MortgageData\",\n",
    "          \"insurance_rate\": 0.003,\n",
    "          \"property_tax_rate\": 0.0125,\n",
    "          \"average_rates\": [\n",
    "            {\n",
    "              \"__typename\": \"Rate\",\n",
    "              \"loan_type\": {\n",
    "                \"__typename\": \"LoanType\",\n",
    "                \"loan_id\": \"thirty_year_fix\",\n",
    "                \"term\": 30,\n",
    "                \"display_name\": \"30-year fixed\",\n",
    "                \"is_va_loan\": null,\n",
    "                \"is_fixed\": true\n",
    "              },\n",
    "              \"rate\": 0.06976\n",
    "            },\n",
    "            {\n",
    "              \"__typename\": \"Rate\",\n",
    "              \"loan_type\": {\n",
    "                \"__typename\": \"LoanType\",\n",
    "                \"loan_id\": \"twenty_year_fix\",\n",
    "                \"term\": 20,\n",
    "                \"display_name\": \"20-year fixed\",\n",
    "                \"is_va_loan\": null,\n",
    "                \"is_fixed\": true\n",
    "              },\n",
    "              \"rate\": 0.06788\n",
    "            },\n",
    "            {\n",
    "              \"__typename\": \"Rate\",\n",
    "              \"loan_type\": {\n",
    "                \"__typename\": \"LoanType\",\n",
    "                \"loan_id\": \"fifteen_year_fix\",\n",
    "                \"term\": 15,\n",
    "                \"display_name\": \"15-year fixed\",\n",
    "                \"is_va_loan\": null,\n",
    "                \"is_fixed\": true\n",
    "              },\n",
    "              \"rate\": 0.06006\n",
    "            },\n",
    "            {\n",
    "              \"__typename\": \"Rate\",\n",
    "              \"loan_type\": {\n",
    "                \"__typename\": \"LoanType\",\n",
    "                \"loan_id\": \"ten_year_fix\",\n",
    "                \"term\": 10,\n",
    "                \"display_name\": \"10-year fixed\",\n",
    "                \"is_va_loan\": null,\n",
    "                \"is_fixed\": true\n",
    "              },\n",
    "              \"rate\": 0.05907\n",
    "            },\n",
    "            {\n",
    "              \"__typename\": \"Rate\",\n",
    "              \"loan_type\": {\n",
    "                \"__typename\": \"LoanType\",\n",
    "                \"loan_id\": \"thirty_year_fha\",\n",
    "                \"term\": 30,\n",
    "                \"display_name\": \"30-year fixed FHA\",\n",
    "                \"is_va_loan\": null,\n",
    "                \"is_fixed\": true\n",
    "              },\n",
    "              \"rate\": 0.06288\n",
    "            },\n",
    "            {\n",
    "              \"__typename\": \"Rate\",\n",
    "              \"loan_type\": {\n",
    "                \"__typename\": \"LoanType\",\n",
    "                \"loan_id\": \"thirty_year_va\",\n",
    "                \"term\": 30,\n",
    "                \"display_name\": \"30-year fixed VA\",\n",
    "                \"is_va_loan\": true,\n",
    "                \"is_fixed\": true\n",
    "              },\n",
    "              \"rate\": 0.0638\n",
    "            },\n",
    "            {\n",
    "              \"__typename\": \"Rate\",\n",
    "              \"loan_type\": {\n",
    "                \"__typename\": \"LoanType\",\n",
    "                \"loan_id\": \"five_one_arm\",\n",
    "                \"term\": 30,\n",
    "                \"display_name\": \"5-year ARM\",\n",
    "                \"is_va_loan\": null,\n",
    "                \"is_fixed\": false\n",
    "              },\n",
    "              \"rate\": 0.07021\n",
    "            },\n",
    "            {\n",
    "              \"__typename\": \"Rate\",\n",
    "              \"loan_type\": {\n",
    "                \"__typename\": \"LoanType\",\n",
    "                \"loan_id\": \"seven_one_arm\",\n",
    "                \"term\": 30,\n",
    "                \"display_name\": \"7-year ARM\",\n",
    "                \"is_va_loan\": null,\n",
    "                \"is_fixed\": false\n",
    "              },\n",
    "              \"rate\": 0.07096\n",
    "            }\n",
    "          ]\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
